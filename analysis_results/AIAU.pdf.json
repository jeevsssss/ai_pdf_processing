{
    "summary": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task. It establishes a new single-model state-of-the-art score of 41.8 after 3.5 days on eight GPUs, a small fraction of the training costs of the literature.",
    "negative_feedback": [
        "This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial.",
        "[3.2.2] While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n\n \n\n",
        "Outrageously large neural networks: The sparsely-gated mixture-of-experts\n\nlayer.",
        "Numerous\nefforts have sin tinued to push the boundaries of recurrent language models and encoder-decoder\narchitectures\n\n \n\n \n\n \n\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences."
    ],
    "keywords": "translation tasks, training parsing, sequence learning, decoder attention, parser training",
    "organizations": [
        "cs.toronto.edu",
        "google.",
        "Google Research University of Toronto Google Brain"
    ]
}